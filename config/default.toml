# Default LexiconWeaver Configuration
# This file serves as a template. Copy to ~/.config/lexiconweaver/config.toml (Linux/Mac)
# or %APPDATA%/lexiconweaver/config.toml (Windows) and customize.

[ollama]
# Ollama server URL
url = "http://localhost:11434"
# Model name to use for translation
model = "llama3.1"
# Request timeout in seconds
timeout = 300
# Maximum retries for failed requests
max_retries = 3

[deepseek]
# DeepSeek API key - set via LEXICONWEAVER_DEEPSEEK__API_KEY environment variable for security
api_key = ""
# DeepSeek model name (V3 is "deepseek-chat")
model = "deepseek-chat"
# DeepSeek API base URL
base_url = "https://api.deepseek.com"
# Request timeout in seconds
timeout = 120
# Maximum retries for failed requests
max_retries = 3

[provider]
# Primary LLM provider: "ollama" or "deepseek"
primary = "ollama"
# Fallback provider when primary fails: "ollama", "deepseek", or "none"
fallback = "none"
# Use fallback on provider errors
fallback_on_error = true

[database]
# Path to the SQLite database file
# If relative, will be resolved relative to the project directory
# If not specified, defaults to ~/.local/share/lexiconweaver/lexiconweaver.db
path = ""

[logging]
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
level = "INFO"
# Enable JSON logging format (for production)
json_logging = false
# Log file path (leave empty for no file logging)
log_file = ""

[cache]
# Enable translation caching
enabled = true
# Maximum cache size (number of entries)
max_size = 10000

[scout]
# Minimum confidence score (0.0-1.0) for a term to be suggested
min_confidence = 0.3
# Maximum N-gram size for term extraction
max_ngram_size = 4
# Enable POS tagging filter (requires spacy model)
use_pos_filter = true
# Enable two-pass LLM refinement for Smart Scout
use_llm_refinement = true
# Number of terms per LLM API call (for batching performance)
llm_batch_size = 50

[weaver]
# Number of paragraphs to process in parallel (0 = sequential)
# parallel_paragraphs = 0   //Not implemented yet
# Enable streaming responses from LLM
streaming = true
# Maximum context window size (in tokens, approximate)
max_context_tokens = 4096
translation_batch_max_chars = 3500
translation_context_sentences = 2
